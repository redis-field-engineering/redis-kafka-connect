/*
 * Copyright Â© 2021 Redis
 * <p>
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 * <p>
 * http://www.apache.org/licenses/LICENSE-2.0
 * <p>
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.redis.kafka.connect.sink;

import com.fasterxml.jackson.annotation.JsonInclude;
import com.fasterxml.jackson.core.JsonProcessingException;
import com.fasterxml.jackson.databind.DeserializationFeature;
import com.fasterxml.jackson.databind.ObjectMapper;
import com.redis.kafka.connect.common.ManifestVersionProvider;
import com.redis.lettucemod.RedisModulesUtils;
import com.redis.lettucemod.api.StatefulRedisModulesConnection;
import com.redis.lettucemod.timeseries.Sample;
import com.redis.spring.batch.item.redis.RedisItemWriter;
import com.redis.spring.batch.item.redis.common.Operation;
import com.redis.spring.batch.item.redis.writer.impl.*;
import com.redis.spring.batch.item.redis.writer.impl.Set;
import io.lettuce.core.*;
import io.lettuce.core.api.async.RedisAsyncCommands;
import io.lettuce.core.codec.ByteArrayCodec;
import org.apache.kafka.clients.consumer.OffsetAndMetadata;
import org.apache.kafka.common.TopicPartition;
import org.apache.kafka.common.config.ConfigException;
import org.apache.kafka.connect.data.Field;
import org.apache.kafka.connect.data.Struct;
import org.apache.kafka.connect.errors.ConnectException;
import org.apache.kafka.connect.errors.DataException;
import org.apache.kafka.connect.errors.RetriableException;
import org.apache.kafka.connect.header.Header;
import org.apache.kafka.connect.json.JsonConverter;
import org.apache.kafka.connect.sink.SinkRecord;
import org.apache.kafka.connect.sink.SinkTask;
import org.apache.kafka.connect.storage.Converter;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.springframework.batch.item.Chunk;
import org.springframework.batch.item.ExecutionContext;
import org.springframework.util.CollectionUtils;

import java.util.*;
import java.util.Map.Entry;
import java.util.function.Predicate;
import java.util.stream.Collector;
import java.util.stream.Collectors;

public class RedisSinkTask extends SinkTask {

    private static final Logger log = LoggerFactory.getLogger(RedisSinkTask.class);

    private static final String OFFSET_KEY_FORMAT = "com.redis.kafka.connect.sink.offset.%s.%s";

    private static final ObjectMapper objectMapper = objectMapper();

    private static final Collector<SinkOffsetState, ?, Map<String, String>> offsetCollector = Collectors
            .toMap(RedisSinkTask::offsetKey, RedisSinkTask::offsetValue);

    private RedisSinkConfig config;

    private AbstractRedisClient client;
    private StatefulRedisModulesConnection<String, String> connection;
    private Converter jsonConverter;
    private RedisItemWriter<byte[], byte[], SinkRecord> writer;

    private static ObjectMapper objectMapper() {
        ObjectMapper mapper = new ObjectMapper();
        mapper.setSerializationInclusion(JsonInclude.Include.NON_NULL);
        mapper.configure(DeserializationFeature.USE_BIG_DECIMAL_FOR_FLOATS, true);
        mapper.configure(DeserializationFeature.USE_LONG_FOR_INTS, true);
        return mapper;
    }

    private static String offsetKey(String topic, Integer partition) {
        return String.format(OFFSET_KEY_FORMAT, topic, partition);
    }

    private static String offsetKey(SinkOffsetState state) {
        return offsetKey(state.topic(), state.partition());
    }

    private static String offsetValue(SinkOffsetState state) {
        try {
            return objectMapper.writeValueAsString(state);
        } catch (JsonProcessingException e) {
            throw new DataException("Could not serialize sink offset state", e);
        }
    }

    @Override
    public String version() {
        return ManifestVersionProvider.getVersion();
    }

    @Override
    public void start(final Map<String, String> props) {
        config = new RedisSinkConfig(props);
        jsonConverter = new JsonConverter();
        jsonConverter.configure(Collections.singletonMap("schemas.enable", "false"), false);
        client = config.client();
        connection = RedisModulesUtils.connection(client);
        writer = RedisItemWriter.operation(ByteArrayCodec.INSTANCE, new ConditionalDel(operation(), del()));
        writer.setClient(client);
        writer.setMultiExec(config.isMultiExec());
        writer.setWaitReplicas(config.getWaitReplicas());
        writer.setWaitTimeout(config.getWaitTimeout());
        writer.setPoolSize(config.getPoolSize());
        writer.open(new ExecutionContext());
        java.util.Set<TopicPartition> assignment = this.context.assignment();
        if (CollectionUtils.isEmpty(assignment)) {
            return;
        }
        Map<TopicPartition, Long> partitionOffsets = new HashMap<>(assignment.size());
        for (SinkOffsetState state : offsetStates(assignment)) {
            partitionOffsets.put(state.topicPartition(), state.offset());
            log.info("Requesting offset {} for {}", state.offset(), state.topicPartition());
        }
        for (TopicPartition topicPartition : assignment) {
            partitionOffsets.putIfAbsent(topicPartition, 0L);
        }
        this.context.offset(partitionOffsets);
    }

    private Operation<byte[], byte[], SinkRecord, Object> del() {
        switch (config.getType()) {
            case HASH:
            case JSON:
            case STRING:
                return new Del<>(this::key);
            default:
                return new Noop<>();
        }
    }

    private Collection<SinkOffsetState> offsetStates(java.util.Set<TopicPartition> assignment) {
        String[] partitionKeys = assignment.stream().map(this::offsetKey).toArray(String[]::new);
        List<KeyValue<String, String>> values = connection.sync().mget(partitionKeys);
        return values.stream().filter(KeyValue::hasValue).map(this::offsetState).collect(Collectors.toList());
    }

    private String offsetKey(TopicPartition partition) {
        return offsetKey(partition.topic(), partition.partition());
    }

    private SinkOffsetState offsetState(KeyValue<String, String> value) {
        try {
            return objectMapper.readValue(value.getValue(), SinkOffsetState.class);
        } catch (JsonProcessingException e) {
            throw new DataException("Could not parse sink offset state", e);
        }
    }

    private Operation<byte[], byte[], SinkRecord, Object> operation() {
        Operation<byte[], byte[], SinkRecord, Object> oper;

        switch (config.getType()) {
            case HASH -> oper = new Hset<>(this::key, this::map);
            case JSON -> oper = new JsonSet<>(this::key, this::jsonValue);
            case STRING -> oper = new Set<>(this::key, this::value);
            case STREAM -> oper = new Xadd<>(this::collectionKey, this::streamMessages);
            case LIST -> oper = new Rpush<>(this::collectionKey, this::members);
            case SET -> oper = new Sadd<>(this::collectionKey, this::members);
            case TIMESERIES -> oper = new TsAdd<>(this::collectionKey, this::samples);
            case ZSET -> oper = new Zadd<>(this::collectionKey, this::scoredValues);
            default -> throw new ConfigException(RedisSinkConfigDef.TYPE_CONFIG, config.getType());
        }

        return new TTLSupport(oper);
    }

    private class TTLSupport implements Operation<byte[], byte[], SinkRecord, Object> {
        private final Operation<byte[], byte[], SinkRecord, Object> delegate;

        public TTLSupport(Operation<byte[], byte[], SinkRecord, Object> delegate) {
            this.delegate = delegate;
        }

        @Override
        public List<RedisFuture<Object>> execute(RedisAsyncCommands<byte[], byte[]> commands, Chunk<? extends SinkRecord> items) {
            List<RedisFuture<Object>> futures = new ArrayList<>(delegate.execute(commands, items));

            for (SinkRecord record : items) {
                byte[] key = determineKey(record);
                long ttlFromRecord = extractTTLFromHeaders(record);

                if (ttlFromRecord > 0) {
                    futures.add((RedisFuture<Object>) (RedisFuture<?>) commands.expire(key, ttlFromRecord));
                } else if (config.getKeyTTL() > 0) {
                    futures.add((RedisFuture<Object>) (RedisFuture<?>) commands.expire(key, config.getKeyTTL()));
                }
            }

            return futures;
        }

        private byte[] determineKey(SinkRecord record) {
            return switch (config.getType()) {
                case STREAM, LIST, SET, ZSET, TIMESERIES -> collectionKey(record);
                default -> key(record);
            };
        }

        private long extractTTLFromHeaders(SinkRecord record) {
            for (Header header : record.headers()) {
                if (header.key().equals(RedisSinkConfigDef.KEY_TTL_CONFIG)) {
                    try {
                        return Long.parseLong(header.value().toString());
                    } catch (NumberFormatException e) {
                        log.warn("Invalid TTL header value for record {}: {}", record, header.value());
                    }
                }
            }
            return -1L;
        }
    }

    private Collection<ScoredValue<byte[]>> scoredValues(SinkRecord sinkRecord) {
        return Arrays.asList(ScoredValue.just(doubleValue(sinkRecord), member(sinkRecord)));
    }

    private Collection<Sample> samples(SinkRecord sinkRecord) {
        return Arrays.asList(Sample.of(longMember(sinkRecord), doubleValue(sinkRecord)));
    }

    private byte[] value(SinkRecord sinkRecord) {
        return bytes("value", sinkRecord.value());
    }

    private byte[] jsonValue(SinkRecord sinkRecord) {
        Object value = sinkRecord.value();
        if (value instanceof byte[]) {
            return (byte[]) value;
        }
        if (value instanceof String) {
            return ((String) value).getBytes(config.getCharset());
        }
        return jsonConverter.fromConnectData(sinkRecord.topic(), sinkRecord.valueSchema(), value);
    }

    private Long longMember(SinkRecord sinkRecord) {
        Object key = sinkRecord.key();
        if (key == null) {
            return null;
        }
        if (key instanceof Number) {
            return ((Number) key).longValue();
        }
        throw new DataException(
                "The key for the record must be a number. Consider using a single message transformation to transform the data before it is written to Redis.");
    }

    private Double doubleValue(SinkRecord sinkRecord) {
        Object value = sinkRecord.value();
        if (value == null) {
            return null;
        }
        if (value instanceof Number) {
            return ((Number) value).doubleValue();
        }
        throw new DataException(
                "The value for the record must be a number. Consider using a single message transformation to transform the data before it is written to Redis.");
    }

    private byte[] key(SinkRecord sinkRecord) {
        if (config.getKeyspace().isEmpty()) {
            return bytes("key", sinkRecord.key());
        }
        String keyspace = keyspace(sinkRecord);
        String key = keyspace + config.getSeparator() + String.valueOf(sinkRecord.key());
        return key.getBytes(config.getCharset());
    }

    private Collection<byte[]> members(SinkRecord sinkRecord) {
        return Arrays.asList(member(sinkRecord));
    }

    private byte[] member(SinkRecord sinkRecord) {
        return bytes("key", sinkRecord.key());
    }

    private String keyspace(SinkRecord sinkRecord) {
        return config.getKeyspace().replace(RedisSinkConfigDef.TOKEN_TOPIC, sinkRecord.topic());
    }

    private byte[] bytes(String source, Object input) {
        if (input instanceof byte[]) {
            return (byte[]) input;
        }
        if (input instanceof String) {
            return ((String) input).getBytes(config.getCharset());
        }
        throw new DataException(String.format(
                "The %s for the record must be a string or byte array. Consider using the StringConverter or ByteArrayConverter if the data is stored in Kafka in the format needed in Redis.",
                source));
    }

    private byte[] collectionKey(SinkRecord sinkRecord) {
        return keyspace(sinkRecord).getBytes(config.getCharset());
    }

    private Collection<StreamMessage<byte[], byte[]>> streamMessages(SinkRecord sinkRecord) {
        return Arrays.asList(new StreamMessage<>(collectionKey(sinkRecord), null, map(sinkRecord)));
    }

    @SuppressWarnings("unchecked")
    private Map<byte[], byte[]> map(SinkRecord sinkRecord) {
        Object value = sinkRecord.value();
        if (value instanceof Struct) {
            Map<byte[], byte[]> body = new LinkedHashMap<>();
            Struct struct = (Struct) value;
            for (Field field : struct.schema().fields()) {
                Object fieldValue = struct.get(field);
                body.put(field.name().getBytes(config.getCharset()),
                        fieldValue == null ? null : fieldValue.toString().getBytes(config.getCharset()));
            }
            return body;
        }
        if (value instanceof Map) {
            Map<String, Object> map = (Map<String, Object>) value;
            Map<byte[], byte[]> body = new LinkedHashMap<>();
            for (Map.Entry<String, Object> e : map.entrySet()) {
                body.put(e.getKey().getBytes(config.getCharset()),
                        String.valueOf(e.getValue()).getBytes(config.getCharset()));
            }
            return body;
        }
        if (value == null) {
			    return Collections.emptyMap();
		    }
        throw new ConnectException("Unsupported source value type: " + sinkRecord.valueSchema().type().name());
    }

    @Override
    public void stop() {
        if (writer != null) {
            writer.close();
        }
        if (connection != null) {
            connection.close();
        }
        if (client != null) {
            client.shutdown();
            client.getResources().shutdown();
        }
    }

    @Override
    public void put(final Collection<SinkRecord> records) {
        log.debug("Processing {} records", records.size());
        try {
            writer.write(new Chunk<>(new ArrayList<>(records)));
        } catch (RedisConnectionException e) {
            throw new RetriableException("Could not get connection to Redis", e);
        } catch (RedisCommandTimeoutException e) {
            throw new RetriableException("Timeout while writing sink records", e);
        } catch (Exception e) {
            throw new RetriableException("Could not write sink records", e);
        }
        log.info("Wrote {} records", records.size());
    }

    @Override
    public void flush(Map<TopicPartition, OffsetAndMetadata> currentOffsets) {
        Map<String, String> offsets = currentOffsets.entrySet().stream().map(this::offsetState)
                .collect(offsetCollector);
        log.trace("Writing offsets: {}", offsets);
        try {
            connection.sync().mset(offsets);
        } catch (RedisCommandTimeoutException e) {
            throw new RetriableException("Could not write offsets", e);
        }
    }

    private SinkOffsetState offsetState(Entry<TopicPartition, OffsetAndMetadata> entry) {
        return SinkOffsetState.of(entry.getKey(), entry.getValue().offset());
    }

    private class ConditionalDel implements Operation<byte[], byte[], SinkRecord, Object> {

        private final Predicate<SinkRecord> delPredicate = r -> r.value() == null;
        private final Operation<byte[], byte[], SinkRecord, Object> write;
        private final Operation<byte[], byte[], SinkRecord, Object> del;

        public ConditionalDel(Operation<byte[], byte[], SinkRecord, Object> delegate,
                              Operation<byte[], byte[], SinkRecord, Object> remove) {
            this.write = delegate;
            this.del = remove;
        }

        @Override
        public List<RedisFuture<Object>> execute(RedisAsyncCommands<byte[], byte[]> commands,
                                                 Chunk<? extends SinkRecord> items) {
            List<RedisFuture<Object>> futures = new ArrayList<>();
            List<SinkRecord> toRemove = items.getItems().stream().filter(delPredicate).collect(Collectors.toList());
            futures.addAll(del.execute(commands, new Chunk<>(toRemove)));
            List<SinkRecord> toWrite = items.getItems().stream().filter(delPredicate.negate())
                    .collect(Collectors.toList());
            futures.addAll(write.execute(commands, new Chunk<>(toWrite)));
            return futures;
        }

    }

}
