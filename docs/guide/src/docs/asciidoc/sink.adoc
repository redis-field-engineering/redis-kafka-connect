:name: Sink Connector
[[_sink]]
= {name}

The {name} consumes records from a Kafka topic and writes the data to Redis.

== Class Name

The sink connector class name is `com.redis.kafka.connect.RedisSinkConnector`.

The corresponding configuration property would be:

[source,properties]
----
connector.class = com.redis.kafka.connect.RedisSinkConnector
----

[[_sink_delivery]]
== At least once delivery
The {name} guarantees that records from the Kafka topic are delivered at least once.

[[_sink_tasks]]
== Tasks

The {name} supports running one or more tasks.
You can specify the number of tasks with the `tasks.max` configuration property.

[[_sink_redis_client]]
include::{includedir}/_redis_client.adoc[leveloffset=+1]

[[_sink_redis_command]]
== Redis Data Structures
The {name} supports the following Redis data-structure types as targets:

* <<_sink_string,string>>
* <<_sink_json,JSON>>
* <<_sink_hash,Hash>>
* <<_sink_stream,stream>>
* <<_sink_list,list>>
* <<_sink_set,set>>
* <<_sink_zset,sorted set>>
* <<_sink_timeseries,time series>>

=== Redis Keys

Redis keys are generated using the `redis.keyspace` configuration property which may contain `${topic}` (default) as a placeholder for the originating topic name.

For `string`, `json`, and `hash` data structures, the Kafka message key will be appended to the keyspace.
For example, with `redis.type = string`, `redis.keyspace = ${topic}`, a message coming from topic `orders` with a key of `123` will translate into a Redis key of `orders:123`.

Leave the keyspace empty ( `redis.keyspace =` ) if you don't want any key prefix and just use the message key.

For `stream`, `list`, `set`, `zset`, and `timeseries` the Redis key is just the keyspace.
For example with `redis.keyspace = set:${topic}` and topic `orders` the Redis key is `set:orders`.

[[_sink_string]]
=== String
Use the following properties to write Kafka records as Redis strings:

[source,properties]
----
redis.type      = STRING
key.converter   = <string or bytes> <1>
value.converter = <string or bytes> <2>
----
<1> <<_sink_key_string,String>> or <<_sink_key_bytes,bytes>>
<2> <<_sink_value_string,String>> or <<_sink_value_bytes,bytes>>.
If value is null the key is deleted.

[[_sink_json]]
=== JSON
Use the following properties to write Kafka records as RedisJSON documents:

[source,properties]
----
redis.type      = JSON
key.converter   = <string, bytes, or Avro> <1>
value.converter = <string, bytes, or Avro> <2>
----
<1> <<_sink_key_string,String>>, <<_sink_key_bytes,bytes>>, or <<_sink_value_avro,Avro>>
<2> <<_sink_value_string,String>>, <<_sink_value_bytes,bytes>>, or <<_sink_value_avro,Avro>>.
If value is null the key is deleted.

[[_sink_hash]]
=== Hash
Use the following properties to write Kafka records as Redis hashes:

[source,properties]
----
redis.type      = HASH
key.converter   = <string or bytes> <1>
value.converter = <Avro or JSON> <2>
----
<1> <<_sink_key_string,String>> or <<_sink_key_bytes,bytes>>
<2> <<_sink_value_avro,Avro>> or <<_sink_value_json,JSON>>.
If value is null the key is deleted.

[[_sink_stream]]
=== Stream
Use the following properties to store Kafka records as Redis stream messages:

[source,properties]
----
redis.type      = STREAM
redis.keyspace  = <stream key> <1>
value.converter = <Avro or JSON> <2>
----
<1> Stream key
<2> <<_sink_value_avro,Avro>> or <<_sink_value_json,JSON>>

[[_sink_list]]
=== List
Use the following properties to add Kafka record keys to a Redis list:

[source,properties]
----
redis.type     = LIST
redis.keyspace = <list key> <2>
key.converter  = <string or bytes> <3>
----

<1> `LPUSH` or `RPUSH`
<2> List key
<3> <<_sink_key_string,String>> or <<_sink_key_bytes,bytes>>: Kafka record keys to push to the list

The Kafka record value can be any format.
If a value is null then the member is removed from the list (instead of pushed to the list).

[[_sink_set]]
=== Set
Use the following properties to add Kafka record keys to a Redis set:

[source,properties]
----
redis.type     = SET
redis.keyspace = <set key> <1>
key.converter  = <string or bytes> <2>
----
<1> Set key
<2> <<_sink_key_string,String>> or <<_sink_key_bytes,bytes>>: Kafka record keys to add to the set

The Kafka record value can be any format.
If a value is null then the member is removed from the set (instead of added to the set).

[[_sink_zset]]
=== Sorted Set
Use the following properties to add Kafka record keys to a Redis sorted set:

[source,properties]
----
redis.type     = ZSET
redis.keyspace = <zset key> <1>
key.converter  = <string or bytes> <2>
----
<1> Sorted set key
<2> <<_sink_key_string,String>> or <<_sink_key_bytes,bytes>>: Kafka record keys to add to the set

The Kafka record value should be `float64` and is used for the score.
If the score is null then the member is removed from the sorted set (instead of added to the sorted set).

[[_sink_timeseries]]
=== Timeseries
Use the following properties to write Kafka records as RedisTimeSeries samples:

[source,properties]
----
redis.type     = TIMESERIES
redis.keyspace = <key name> <1>
----
<1> Timeseries key

The Kafka record key must be an integer (e.g. `int64`) as it is used for the sample time in milliseconds.

The Kafka record value must be a number (e.g. `float64`) as it is used as the sample value.


[[_sink_data_formats]]
== Data Formats

The {name} supports different data formats for record keys and values depending on the target Redis data structure.

[[_sink_key]]
=== Kafka Record Key
The {name} expects Kafka record keys in a specific format depending on the configured target <<_sink_redis_command,Redis data structure>>:

[options="header",cols="h,1,1"]
|====
|Target|Record Key|Assigned To
|Stream|Any|None
|Hash|String|Key
|String|<<_sink_key_string,String>> or <<_sink_key_bytes,bytes>>|Key
|List|<<_sink_key_string,String>> or <<_sink_key_bytes,bytes>>|Member
|Set|<<_sink_key_string,String>> or <<_sink_key_bytes,bytes>>|Member
|Sorted Set|<<_sink_key_string,String>> or <<_sink_key_bytes,bytes>>|Member
|JSON|<<_sink_key_string,String>> or <<_sink_key_bytes,bytes>>|Key
|TimeSeries|Integer|Sample time in milliseconds
|====

[[_sink_key_string]]
==== StringConverter
If record keys are already serialized as strings use the StringConverter:

[source,properties]
----
key.converter = org.apache.kafka.connect.storage.StringConverter
----

[[_sink_key_bytes]]
==== ByteArrayConverter
Use the byte array converter to use the binary serialized form of the Kafka record keys:

[source,properties]
----
key.converter = org.apache.kafka.connect.converters.ByteArrayConverter
----

[[_sink_value]]
=== Kafka Record Value
Multiple data formats are supported for Kafka record values depending on the configured target <<_sink_redis_command,Redis data structure>>.
Each data structure expects a specific format.
If your data in Kafka is not in the format expected for a given data structure, consider using {link_smt} to convert to a byte array, string, Struct, or map before it is written to Redis.

[options="header",cols="h,1,1"]
|====
|Target|Record Value|Assigned To
|Stream|<<_sink_value_avro,Avro>> or <<_sink_value_json,JSON>>|Message body
|Hash|<<_sink_value_avro,Avro>> or <<_sink_value_json,JSON>>|Fields
|String|<<_sink_value_string,String>> or <<_sink_value_bytes,bytes>>|Value
|List|Any|Removal if null
|Set|Any|Removal if null
|Sorted Set|Number|Score or removal if null
|JSON|<<_sink_value_string,String>> or <<_sink_value_bytes,bytes>>|Value
|TimeSeries|Number|Sample value
|====

[[_sink_value_string]]
==== StringConverter
If record values are already serialized as strings, use the StringConverter to store values in Redis as strings:

[source,properties]
----
value.converter = org.apache.kafka.connect.storage.StringConverter
----

[[_sink_value_bytes]]
==== ByteArrayConverter
Use the byte array converter to store the binary serialized form (for example, JSON, Avro, Strings, etc.) of the Kafka record values in Redis as byte arrays:

[source,properties]
----
value.converter = org.apache.kafka.connect.converters.ByteArrayConverter
----

[[_sink_value_avro]]
==== Avro
[source,properties]
----
value.converter                     = io.confluent.connect.avro.AvroConverter
value.converter.schema.registry.url = http://localhost:8081
----

[[_sink_value_json]]
==== JSON
[source,properties]
----
value.converter                = org.apache.kafka.connect.json.JsonConverter
value.converter.schemas.enable = <true|false> <1>
----
<1> Set to `true` if the JSON record structure has an attached schema

[[_sink_ttl]]
== TTL (Time To Live)

The {name} supports setting TTL (Time To Live) for Redis keys to automatically expire data after a specified duration. TTL can be configured globally for all keys or per-message using Kafka headers.

=== Global TTL Configuration

Set a global TTL for all keys using the `redis.key.ttl` configuration property:

[source,properties]
----
redis.key.ttl = 3600  # TTL in seconds (1 hour)
----

=== Per-Message TTL

Include a `redis.key.ttl` header in your Kafka messages to set TTL for individual records:

[source,java]
----
ProducerRecord<String, String> record = new ProducerRecord<>("topic", "key", "value");
record.headers().add("redis.key.ttl", "1800".getBytes()); // 30 minutes
----

=== TTL Behavior

* TTL is applied using the Redis `EXPIRE` command after data is written
* Per-message TTL headers take precedence over global configuration
* TTL applies to all Redis data types
* For collection types (STREAM, LIST, SET, ZSET, TIMESERIES), TTL is applied to the collection key
* Invalid TTL values are logged and ignored
* TTL value of -1 (default) means no expiration

[[_sink_config]]
== Configuration

[source,properties]
----
connector.class  = com.redis.kafka.connect.RedisSinkConnector
topics           = <Kafka topic> <1>
redis.uri        = <Redis URI> <2>
redis.type       = <HASH|SET|JSON|STREAM|LIST|SET|ZSET|TIMESERIES> <3>
redis.key.ttl    = <TTL in seconds> <4>
key.converter    = <Key converter> <5>
value.converter  = <Value converter> <6>
----
<1> Kafka topics to read messsages from.
<2> <<_sink_redis_client,Redis URI>>.
<3> <<_sink_redis_command,Redis command>>.
<4> <<_sink_ttl,TTL in seconds>> (optional, default: -1 for no expiration).
<5> <<_sink_key,Key converter>>.
<6> <<_sink_value,Value converter>>.

